"""
Script de rÃ©entraÃ®nement progressif avec diagnostic
Augmente graduellement la complexitÃ© et les donnÃ©es
"""

import os
import sys
import torch
import json

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training.FineTuning import OASSTTrainer, DPOConfig


def diagnose_model(trainer):
    """Diagnostic rapide du modÃ¨le"""
    print("\n" + "="*70)
    print("ğŸ” DIAGNOSTIC DU MODÃˆLE")
    print("="*70)
    
    # Charger l'historique
    history = trainer.history
    
    print(f"ğŸ“Š Cycles d'entraÃ®nement: {len(history['cycles'])}")
    print(f"ğŸ“š Total exemples vus: {history['total_examples_trained']}")
    print(f"ğŸ¯ DPO cycles: {history['dpo_cycles']}")
    
    if history['cycles']:
        last_cycle = history['cycles'][-1]
        print(f"\nğŸ“ˆ Dernier cycle:")
        print(f"   Loss: {last_cycle['avg_loss']:.4f}")
        print(f"   Exemples: {last_cycle['examples']}")
        print(f"   Ã‰poques: {last_cycle['epochs']}")
    
    # Ã‰valuation
    print(f"\nğŸ’¡ Recommandations:")
    
    if history['total_examples_trained'] < 10000:
        print("   âš ï¸  CRITIQUE: Moins de 10k exemples")
        print("      â†’ EntraÃ®nez avec au moins 10k-20k exemples")
        recommendation = "train_more"
    elif history['total_examples_trained'] < 50000:
        print("   âš ï¸  EntraÃ®nement lÃ©ger (< 50k exemples)")
        print("      â†’ Continuez l'entraÃ®nement supervisÃ©")
        recommendation = "continue"
    else:
        print("   âœ… EntraÃ®nement supervisÃ© suffisant")
        if history['dpo_cycles'] == 0:
            print("      â†’ Passez au DPO pour l'alignment")
            recommendation = "dpo"
        else:
            print("      â†’ ModÃ¨le bien entraÃ®nÃ©!")
            recommendation = "done"
    
    print("="*70)
    return recommendation


def test_generation(trainer, prompts):
    """Test rapide de gÃ©nÃ©ration"""
    print("\n" + "="*70)
    print("ğŸ§ª TEST DE GÃ‰NÃ‰RATION")
    print("="*70)
    
    trainer.model.eval()
    
    for prompt in prompts:
        print(f"\nğŸ‘¤ Prompt: {prompt}")
        print("ğŸ¤– GÃ©nÃ©ration: ", end='', flush=True)
        
        # Encoder
        formatted = f"[INST] {prompt} [/INST]"
        input_ids = trainer.tokenizer.encode(formatted, add_special_tokens=False)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(trainer.device)
        
        # GÃ©nÃ©rer
        with torch.no_grad():
            generated = []
            current = input_tensor
            
            for _ in range(50):  # 50 tokens max
                logits, _ = trainer.model(current)
                next_logits = logits[0, -1, :]
                
                # Temperature + sampling
                next_logits = next_logits / 0.7
                probs = torch.softmax(next_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                if next_token.item() == trainer.tokenizer.eos_token_id:
                    break
                
                generated.append(next_token.item())
                current = torch.cat([current, next_token.unsqueeze(0)], dim=1)
            
            if generated:
                response = trainer.tokenizer.decode(generated, skip_special_tokens=True)
                print(response[:200])
            else:
                print("[AUCUNE GÃ‰NÃ‰RATION]")
    
    print("\n" + "="*70)


def progressive_training(trainer, phase):
    """EntraÃ®nement progressif selon la phase"""
    
    if phase == "phase1":
        print("\n" + "="*70)
        print("ğŸ“š PHASE 1: Foundation (10k exemples)")
        print("="*70)
        trainer.train_one_cycle(
            num_oasst1=5000,
            num_oasst2=5000,
            epochs=3,
            batch_size=8,
            lr=5e-5
        )
    
    elif phase == "phase2":
        print("\n" + "="*70)
        print("ğŸ“š PHASE 2: Renforcement (20k exemples)")
        print("="*70)
        trainer.train_one_cycle(
            num_oasst1=10000,
            num_oasst2=10000,
            epochs=4,
            batch_size=8,
            lr=3e-5  # LR plus petit
        )
    
    elif phase == "phase3":
        print("\n" + "="*70)
        print("ğŸ¯ PHASE 3: DPO Alignment")
        print("="*70)
        trainer.train_with_dpo(
            max_samples=10000,
            epochs=2,
            batch_size=4,
            lr=5e-7,
            beta=0.1
        )
    
    elif phase == "phase4":
        print("\n" + "="*70)
        print("ğŸ¯ PHASE 4: DPO Fine-tuning")
        print("="*70)
        trainer.train_with_dpo(
            max_samples=20000,
            epochs=1,
            batch_size=4,
            lr=1e-7,  # TrÃ¨s petit LR pour fine-tuning
            beta=0.15
        )


def main():
    print("\n" + "="*70)
    print("ğŸ”„ RÃ‰ENTRAÃNEMENT PROGRESSIF")
    print("="*70)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» Device: {device}")
    
    model_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "saved_models",
        "my_llm"
    )
    
    tokenizer_name = "mistralai/Mistral-7B-v0.1"
    
    # CrÃ©er le trainer
    trainer = OASSTTrainer(
        model_dir=model_dir,
        tokenizer_name=tokenizer_name,
        device=device,
        language='en',
        instruction_template=None
    )
    
    # Diagnostic
    recommendation = diagnose_model(trainer)
    
    # Test de gÃ©nÃ©ration avant
    print("\nğŸ“Š TEST AVANT ENTRAÃNEMENT:")
    test_prompts = [
        "Hello!",
        "What is AI?",
        "Tell me a joke"
    ]
    test_generation(trainer, test_prompts)
    
    # Demander confirmation
    print("\n" + "="*70)
    print("ğŸ¤” PLAN D'ENTRAÃNEMENT RECOMMANDÃ‰:")
    print("="*70)
    
    if recommendation == "train_more":
        print("âœ… Phase 1: Foundation (10k exemples, 3 Ã©poques)")
        print("âœ… Phase 2: Renforcement (20k exemples, 4 Ã©poques)")
        print("âœ… Phase 3: DPO Alignment (10k exemples, 2 Ã©poques)")
        print("âœ… Phase 4: DPO Fine-tuning (20k exemples, 1 Ã©poque)")
        phases = ["phase1", "phase2", "phase3", "phase4"]
    
    elif recommendation == "continue":
        print("âœ… Phase 2: Renforcement (20k exemples, 4 Ã©poques)")
        print("âœ… Phase 3: DPO Alignment (10k exemples, 2 Ã©poques)")
        print("âœ… Phase 4: DPO Fine-tuning (20k exemples, 1 Ã©poque)")
        phases = ["phase2", "phase3", "phase4"]
    
    elif recommendation == "dpo":
        print("âœ… Phase 3: DPO Alignment (10k exemples, 2 Ã©poques)")
        print("âœ… Phase 4: DPO Fine-tuning (20k exemples, 1 Ã©poque)")
        phases = ["phase3", "phase4"]
    
    else:
        print("âœ… ModÃ¨le dÃ©jÃ  bien entraÃ®nÃ©!")
        print("ğŸ’¡ Vous pouvez faire du fine-tuning additionnel si nÃ©cessaire")
        phases = []
    
    if phases:
        print(f"\nâ±ï¸  Temps estimÃ©: {len(phases) * 30}-{len(phases) * 60} minutes")
        print("="*70)
        
        response = input("\nğŸš€ Lancer l'entraÃ®nement progressif? (y/N): ")
        
        if response.lower() == 'y':
            for i, phase in enumerate(phases, 1):
                print(f"\n{'='*70}")
                print(f"ğŸ¯ Ã‰TAPE {i}/{len(phases)}")
                print('='*70)
                
                progressive_training(trainer, phase)
                
                # Test intermÃ©diaire
                if i < len(phases):
                    print(f"\nğŸ“Š TEST APRÃˆS PHASE {i}:")
                    test_generation(trainer, ["Hello!", "What is AI?"])
            
            # Test final
            print("\n" + "="*70)
            print("ğŸ‰ ENTRAÃNEMENT PROGRESSIF TERMINÃ‰!")
            print("="*70)
            print("\nğŸ“Š TEST FINAL:")
            test_generation(trainer, test_prompts)
            
            print("\nğŸ’¡ Testez en mode interactif avec:")
            print("   python test.py --mode interactive --template mistral")
        else:
            print("\nâŒ EntraÃ®nement annulÃ©")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    main()