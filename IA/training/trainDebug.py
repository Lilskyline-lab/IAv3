"""
Script d'exp√©rimentation pour Google Colab
4 exp√©riences pour identifier la source du probl√®me

EXP√âRIENCES:
1. BASELINE     : Train supervis√© simple (OASST) SANS instruction tuning NI DPO
2. WITH_IT      : Train supervis√© AVEC instruction tuning, SANS DPO
3. WITH_DPO     : Train supervis√© simple + DPO
4. FULL         : Train complet (supervis√© + instruction tuning + DPO)

Usage dans Colab:
    !git clone [votre_repo]
    %cd [repo]/IA
    !python colab_experiments.py --experiment all --gpu
"""

import os
import sys
import json
import time
import torch
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from Model.HessGPT import HessGPT
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
from tqdm import tqdm


class SimpleDataset(Dataset):
    """Dataset minimal sans instruction tuning"""
    def __init__(self, pairs, tokenizer, max_length=256):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        pair = self.pairs[idx]
        # Format simple: juste concat√©ner
        text = f"Human: {pair['human']}\nAssistant: {pair['assistant']}"
        
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        if len(tokens) > self.max_length:
            tokens = tokens[-self.max_length:]
        
        # Tout est consid√©r√© comme r√©ponse (pas de masking)
        return {
            "input_ids": torch.tensor(tokens, dtype=torch.long),
            "assist_start": 0  # Tout est loss
        }


def collate_simple(batch, pad_id=2):
    """Collate simple"""
    input_ids_list = [b["input_ids"] for b in batch]
    max_len = max(t.size(0) for t in input_ids_list)
    
    input_ids = torch.full((len(batch), max_len), pad_id, dtype=torch.long)
    labels = torch.full((len(batch), max_len), -100, dtype=torch.long)
    
    for i, ids in enumerate(input_ids_list):
        L = ids.size(0)
        input_ids[i, :L] = ids
        # Tout est cible (pas de masking intelligent)
        labels[i, :L] = ids
    
    return {"input_ids": input_ids, "labels": labels}


def create_synthetic_data(num_samples=1000):
    """Cr√©e des donn√©es synth√©tiques pour le test"""
    templates = [
        ("Hello", "Hello! How can I help you?"),
        ("Hi", "Hi there! What can I do for you?"),
        ("What is AI?", "AI is artificial intelligence."),
        ("What is Python?", "Python is a programming language."),
        ("Explain ML", "Machine learning is a subset of AI."),
        ("Tell me about coding", "Coding is writing instructions for computers."),
        ("What is data?", "Data is information stored digitally."),
        ("Define algorithm", "An algorithm is a set of instructions."),
        ("What is a model?", "A model is a trained AI system."),
        ("Explain training", "Training is teaching a model from data."),
    ]
    
    data = []
    for _ in range(num_samples):
        template = templates[len(data) % len(templates)]
        data.append({
            "human": template[0],
            "assistant": template[1]
        })
    
    return data


def train_baseline(
    model_dir: str,
    tokenizer_name: str,
    device: torch.device,
    num_samples: int = 1000,
    epochs: int = 5,
    batch_size: int = 16
):
    """
    EXP√âRIENCE 1: BASELINE
    Train supervis√© simple SANS instruction tuning NI DPO
    """
    print("\n" + "="*80)
    print("üß™ EXP√âRIENCE 1: BASELINE (Supervis√© simple)")
    print("="*80)
    print("üìã Configuration:")
    print(f"   - Samples: {num_samples}")
    print(f"   - Epochs: {epochs}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Instruction tuning: ‚ùå NON")
    print(f"   - DPO: ‚ùå NON")
    print("="*80)
    
    from transformers import AutoTokenizer
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    vocab_size = len(tokenizer)
    
    # Mod√®le
    model = HessGPT(
        vocab_size=vocab_size,
        embed_dim=512,
        num_heads=8,
        num_layers=6,
        max_seq_len=256
    ).to(device)
    
    # Donn√©es synth√©tiques
    data = create_synthetic_data(num_samples)
    dataset = SimpleDataset(data, tokenizer, max_length=256)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_simple(b, pad_id=tokenizer.pad_token_id or 2)
    )
    
    # Training
    optimizer = AdamW(model.parameters(), lr=5e-5)
    loss_fn = CrossEntropyLoss(ignore_index=-100)
    
    model.train()
    losses = []
    
    for epoch in range(epochs):
        epoch_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in pbar:
            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)
            
            logits, _ = model(input_ids)
            loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            epoch_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)
        print(f"‚úì Epoch {epoch+1}: Loss = {avg_loss:.4f}")
    
    # Sauvegarder
    exp_dir = os.path.join(model_dir, "exp1_baseline")
    os.makedirs(exp_dir, exist_ok=True)
    
    torch.save(model.state_dict(), os.path.join(exp_dir, "model.pt"))
    
    with open(os.path.join(exp_dir, "results.json"), 'w') as f:
        json.dump({
            "experiment": "baseline",
            "losses": losses,
            "final_loss": losses[-1],
            "config": {
                "samples": num_samples,
                "epochs": epochs,
                "batch_size": batch_size,
                "instruction_tuning": False,
                "dpo": False
            }
        }, f, indent=2)
    
    print(f"\n‚úÖ Exp√©rience 1 termin√©e: Loss finale = {losses[-1]:.4f}")
    print(f"üíæ Sauvegard√© dans: {exp_dir}")
    
    return losses[-1], exp_dir


def train_with_instruction_tuning(
    model_dir: str,
    tokenizer_name: str,
    device: torch.device,
    num_samples: int = 1000,
    epochs: int = 5,
    batch_size: int = 16
):
    """
    EXP√âRIENCE 2: WITH_IT
    Train supervis√© AVEC instruction tuning, SANS DPO
    """
    print("\n" + "="*80)
    print("üß™ EXP√âRIENCE 2: WITH INSTRUCTION TUNING")
    print("="*80)
    print("üìã Configuration:")
    print(f"   - Samples: {num_samples}")
    print(f"   - Epochs: {epochs}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Instruction tuning: ‚úÖ OUI (Mistral format)")
    print(f"   - DPO: ‚ùå NON")
    print("="*80)
    
    from transformers import AutoTokenizer
    from utils.instruction_tuning import convert_to_instruction_format
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    vocab_size = len(tokenizer)
    
    # Mod√®le
    model = HessGPT(
        vocab_size=vocab_size,
        embed_dim=512,
        num_heads=8,
        num_layers=6,
        max_seq_len=256
    ).to(device)
    
    # Donn√©es avec instruction tuning
    data = create_synthetic_data(num_samples)
    formatted_data = convert_to_instruction_format(data, template_name="mistral")
    
    # Dataset custom pour IT
    class ITDataset(Dataset):
        def __init__(self, formatted, tokenizer, max_length=256):
            self.formatted = formatted
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __len__(self):
            return len(self.formatted)
        
        def __getitem__(self, idx):
            text = self.formatted[idx]['formatted_text']
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            
            if len(tokens) > self.max_length:
                tokens = tokens[-self.max_length:]
            
            # Trouver o√π commence [/INST]
            inst_close = "[/INST]"
            inst_tokens = self.tokenizer.encode(inst_close, add_special_tokens=False)
            assist_start = 0
            
            # Simple: consid√©rer 60% du texte comme prompt
            assist_start = int(len(tokens) * 0.6)
            
            return {
                "input_ids": torch.tensor(tokens, dtype=torch.long),
                "assist_start": assist_start
            }
    
    def collate_it(batch, pad_id=2):
        input_ids_list = [b["input_ids"] for b in batch]
        assist_starts = [b["assist_start"] for b in batch]
        max_len = max(t.size(0) for t in input_ids_list)
        
        input_ids = torch.full((len(batch), max_len), pad_id, dtype=torch.long)
        labels = torch.full((len(batch), max_len), -100, dtype=torch.long)
        
        for i, ids in enumerate(input_ids_list):
            L = ids.size(0)
            input_ids[i, :L] = ids
            start = assist_starts[i]
            labels[i, start:L] = ids[start:L]
        
        return {"input_ids": input_ids, "labels": labels}
    
    dataset = ITDataset(formatted_data, tokenizer, max_length=256)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_it(b, pad_id=tokenizer.pad_token_id or 2)
    )
    
    # Training
    optimizer = AdamW(model.parameters(), lr=5e-5)
    loss_fn = CrossEntropyLoss(ignore_index=-100)
    
    model.train()
    losses = []
    
    for epoch in range(epochs):
        epoch_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in pbar:
            input_ids = batch["input_ids"].to(device)
            labels = batch["labels"].to(device)
            
            logits, _ = model(input_ids)
            loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            epoch_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)
        print(f"‚úì Epoch {epoch+1}: Loss = {avg_loss:.4f}")
    
    # Sauvegarder
    exp_dir = os.path.join(model_dir, "exp2_with_it")
    os.makedirs(exp_dir, exist_ok=True)
    
    torch.save(model.state_dict(), os.path.join(exp_dir, "model.pt"))
    
    with open(os.path.join(exp_dir, "results.json"), 'w') as f:
        json.dump({
            "experiment": "with_instruction_tuning",
            "losses": losses,
            "final_loss": losses[-1],
            "config": {
                "samples": num_samples,
                "epochs": epochs,
                "batch_size": batch_size,
                "instruction_tuning": True,
                "dpo": False
            }
        }, f, indent=2)
    
    print(f"\n‚úÖ Exp√©rience 2 termin√©e: Loss finale = {losses[-1]:.4f}")
    print(f"üíæ Sauvegard√© dans: {exp_dir}")
    
    return losses[-1], exp_dir


def train_simple_plus_dpo(
    model_dir: str,
    tokenizer_name: str,
    device: torch.device,
    num_samples: int = 1000,
    epochs: int = 5,
    batch_size: int = 16
):
    """
    EXP√âRIENCE 3: WITH_DPO
    Train supervis√© simple + DPO (SANS instruction tuning)
    """
    print("\n" + "="*80)
    print("üß™ EXP√âRIENCE 3: SIMPLE + DPO")
    print("="*80)
    print("üìã Configuration:")
    print(f"   - Samples supervis√©: {num_samples}")
    print(f"   - Epochs supervis√©: {epochs}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Instruction tuning: ‚ùå NON")
    print(f"   - DPO: ‚úÖ OUI (ultra-light)")
    print("="*80)
    
    # Phase 1: Train baseline
    print("\nüìö Phase 1: Entra√Ænement supervis√© simple...")
    final_loss, baseline_dir = train_baseline(
        model_dir, tokenizer_name, device,
        num_samples, epochs, batch_size
    )
    
    # Phase 2: DPO ultra-light
    print("\nüéØ Phase 2: DPO ultra-light...")
    print("‚ö†Ô∏è  DPO sur CPU est tr√®s lent, utilisation de mini-batch")
    
    from utils.rlhf_module import train_with_dpo, DPOConfig
    from transformers import AutoTokenizer
    
    # Charger le mod√®le baseline
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    vocab_size = len(tokenizer)
    
    model = HessGPT(
        vocab_size=vocab_size,
        embed_dim=512,
        num_heads=8,
        num_layers=6,
        max_seq_len=256
    ).to(device)
    
    model.load_state_dict(torch.load(os.path.join(baseline_dir, "model.pt")))
    
    # DPO config ultra-light
    dpo_config = DPOConfig(
        max_samples_train=50,  # Tr√®s peu de samples
        max_samples_val=10,
        batch_size=2,          # Tr√®s petit batch
        num_epochs=1,
        learning_rate=5e-7,
        beta=0.1,
        output_dir=os.path.join(model_dir, "exp3_with_dpo", "dpo_temp")
    )
    
    try:
        train_with_dpo(
            model=model,
            tokenizer=tokenizer,
            device=device,
            config=dpo_config,
            model_dir=model_dir
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  DPO √©chou√© (normal sur CPU): {e}")
        print("üí° Utilisant le mod√®le baseline sans DPO")
    
    # Sauvegarder
    exp_dir = os.path.join(model_dir, "exp3_with_dpo")
    os.makedirs(exp_dir, exist_ok=True)
    
    torch.save(model.state_dict(), os.path.join(exp_dir, "model.pt"))
    
    with open(os.path.join(exp_dir, "results.json"), 'w') as f:
        json.dump({
            "experiment": "simple_plus_dpo",
            "supervised_loss": final_loss,
            "config": {
                "samples": num_samples,
                "epochs": epochs,
                "batch_size": batch_size,
                "instruction_tuning": False,
                "dpo": True
            }
        }, f, indent=2)
    
    print(f"\n‚úÖ Exp√©rience 3 termin√©e")
    print(f"üíæ Sauvegard√© dans: {exp_dir}")
    
    return final_loss, exp_dir


def train_full_pipeline(
    model_dir: str,
    tokenizer_name: str,
    device: torch.device,
    num_samples: int = 1000,
    epochs: int = 5,
    batch_size: int = 16
):
    """
    EXP√âRIENCE 4: FULL
    Train complet: supervis√© + instruction tuning + DPO
    """
    print("\n" + "="*80)
    print("üß™ EXP√âRIENCE 4: FULL PIPELINE")
    print("="*80)
    print("üìã Configuration:")
    print(f"   - Samples supervis√©: {num_samples}")
    print(f"   - Epochs supervis√©: {epochs}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Instruction tuning: ‚úÖ OUI")
    print(f"   - DPO: ‚úÖ OUI")
    print("="*80)
    
    # Phase 1: Supervis√© + IT
    print("\nüìö Phase 1: Supervis√© avec Instruction Tuning...")
    it_loss, it_dir = train_with_instruction_tuning(
        model_dir, tokenizer_name, device,
        num_samples, epochs, batch_size
    )
    
    # Phase 2: DPO
    print("\nüéØ Phase 2: DPO ultra-light...")
    
    from utils.rlhf_module import train_with_dpo, DPOConfig
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    vocab_size = len(tokenizer)
    
    model = HessGPT(
        vocab_size=vocab_size,
        embed_dim=512,
        num_heads=8,
        num_layers=6,
        max_seq_len=256
    ).to(device)
    
    model.load_state_dict(torch.load(os.path.join(it_dir, "model.pt")))
    
    dpo_config = DPOConfig(
        max_samples_train=50,
        max_samples_val=10,
        batch_size=2,
        num_epochs=1,
        learning_rate=5e-7,
        beta=0.1,
        output_dir=os.path.join(model_dir, "exp4_full", "dpo_temp")
    )
    
    try:
        train_with_dpo(
            model=model,
            tokenizer=tokenizer,
            device=device,
            config=dpo_config,
            model_dir=model_dir
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  DPO √©chou√©: {e}")
    
    # Sauvegarder
    exp_dir = os.path.join(model_dir, "exp4_full")
    os.makedirs(exp_dir, exist_ok=True)
    
    torch.save(model.state_dict(), os.path.join(exp_dir, "model.pt"))
    
    with open(os.path.join(exp_dir, "results.json"), 'w') as f:
        json.dump({
            "experiment": "full_pipeline",
            "supervised_loss": it_loss,
            "config": {
                "samples": num_samples,
                "epochs": epochs,
                "batch_size": batch_size,
                "instruction_tuning": True,
                "dpo": True
            }
        }, f, indent=2)
    
    print(f"\n‚úÖ Exp√©rience 4 termin√©e")
    print(f"üíæ Sauvegard√© dans: {exp_dir}")
    
    return it_loss, exp_dir


def compare_all_experiments(model_dir: str):
    """Compare les r√©sultats de toutes les exp√©riences"""
    print("\n" + "="*80)
    print("üìä COMPARAISON DES 4 EXP√âRIENCES")
    print("="*80)
    
    experiments = [
        ("exp1_baseline", "BASELINE (Simple)"),
        ("exp2_with_it", "WITH IT (+ Instruction Tuning)"),
        ("exp3_with_dpo", "WITH DPO (+ DPO only)"),
        ("exp4_full", "FULL (IT + DPO)")
    ]
    
    results = []
    
    for exp_dir, exp_name in experiments:
        full_path = os.path.join(model_dir, exp_dir)
        results_file = os.path.join(full_path, "results.json")
        
        if os.path.exists(results_file):
            with open(results_file, 'r') as f:
                data = json.load(f)
                results.append((exp_name, data))
                
                print(f"\nüîπ {exp_name}")
                print(f"   Loss finale: {data.get('final_loss', data.get('supervised_loss', 'N/A')):.4f}")
                print(f"   IT: {'‚úÖ' if data['config']['instruction_tuning'] else '‚ùå'}")
                print(f"   DPO: {'‚úÖ' if data['config']['dpo'] else '‚ùå'}")
        else:
            print(f"\nüîπ {exp_name}: ‚ùå R√©sultats non trouv√©s")
    
    print("\n" + "="*80)
    print("üí° CONCLUSIONS:")
    print("="*80)
    print("""
1. Si BASELINE fonctionne mais WITH_IT √©choue:
   ‚Üí Probl√®me dans l'instruction tuning (formatage, masking)

2. Si BASELINE fonctionne mais WITH_DPO √©choue:
   ‚Üí Probl√®me dans DPO (KL divergence, reference model)

3. Si BASELINE √©choue:
   ‚Üí Probl√®me fondamental (architecture, tokenizer, donn√©es)

4. Si FULL fonctionne le mieux:
   ‚Üí Pipeline correct, continuer avec plus de donn√©es
    """)
    print("="*80)


def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", choices=["1", "2", "3", "4", "all"], default="all")
    parser.add_argument("--model_dir", default="saved_models/experiments")
    parser.add_argument("--tokenizer", default="mistralai/Mistral-7B-v0.1")
    parser.add_argument("--samples", type=int, default=1000)
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--gpu", action="store_true", help="Utiliser GPU")
    
    args = parser.parse_args()
    
    # Device
    if args.gpu and torch.cuda.is_available():
        device = torch.device("cuda")
        print("üöÄ GPU d√©tect√© et activ√©!")
    else:
        device = torch.device("cpu")
        print("üíª Utilisation du CPU")
    
    # Chemin absolu
    if not os.path.isabs(args.model_dir):
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        args.model_dir = os.path.join(base_dir, args.model_dir)
    
    os.makedirs(args.model_dir, exist_ok=True)
    
    print("\n" + "="*80)
    print("üî¨ DIAGNOSTIC SYSTEM - 4 EXP√âRIENCES")
    print("="*80)
    print(f"üìÅ Dossier: {args.model_dir}")
    print(f"üî§ Tokenizer: {args.tokenizer}")
    print(f"üíª Device: {device}")
    print(f"üìä Samples: {args.samples}")
    print(f"üîÅ Epochs: {args.epochs}")
    print("="*80)
    
    start_time = time.time()
    
    try:
        if args.experiment in ["1", "all"]:
            train_baseline(args.model_dir, args.tokenizer, device, 
                          args.samples, args.epochs, args.batch_size)
        
        if args.experiment in ["2", "all"]:
            train_with_instruction_tuning(args.model_dir, args.tokenizer, device,
                                         args.samples, args.epochs, args.batch_size)
        
        if args.experiment in ["3", "all"]:
            train_simple_plus_dpo(args.model_dir, args.tokenizer, device,
                                 args.samples, args.epochs, args.batch_size)
        
        if args.experiment in ["4", "all"]:
            train_full_pipeline(args.model_dir, args.tokenizer, device,
                               args.samples, args.epochs, args.batch_size)
        
        # Comparaison finale
        if args.experiment == "all":
            compare_all_experiments(args.model_dir)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Exp√©riences interrompues")
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
    
    elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è  Temps total: {elapsed/60:.1f} minutes")
    print("\n‚úÖ Exp√©riences termin√©es!")
    print(f"üìÅ R√©sultats dans: {args.model_dir}")


if __name__ == "__main__":
    main()
